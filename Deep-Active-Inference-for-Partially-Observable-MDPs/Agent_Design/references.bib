@book{Active-Inference-Book,
    author = {Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.},
    title = "{Active Inference: The Free Energy Principle in Mind, Brain, and Behavior}",
    publisher = {The MIT Press},
    year = {2022},
    month = {03},
    abstract = "{The first comprehensive treatment of active inference, an integrative perspective on brain, cognition, and behavior used across multiple disciplines.Active inference is a way of understanding sentient behavior—a theory that characterizes perception, planning, and action in terms of probabilistic inference. Developed by theoretical neuroscientist Karl Friston over years of groundbreaking research, active inference provides an integrated perspective on brain, cognition, and behavior that is increasingly used across multiple disciplines including neuroscience, psychology, and philosophy. Active inference puts the action into perception. This book offers the first comprehensive treatment of active inference, covering theory, applications, and cognitive domains. Active inference is a “first principles” approach to understanding behavior and the brain, framed in terms of a single imperative to minimize free energy. The book emphasizes the implications of the free energy principle for understanding how the brain works. It first introduces active inference both conceptually and formally, contextualizing it within current theories of cognition. It then provides specific examples of computational models that use active inference to explain such cognitive phenomena as perception, attention, memory, and planning.}",
    isbn = {9780262369978},
    doi = {10.7551/mitpress/12441.001.0001},
    url = {https://doi.org/10.7551/mitpress/12441.001.0001},
    eprint = {https://direct.mit.edu/book-pdf/2087488/book\_9780262369978.pdf},
}

@book{Cherniak1986,
    title = {Minimal Rationality},
    editor = {Christopher Cherniak},
    author = {Christopher Cherniak},
    year = {1986},
    publisher = {MIT Press}
}

@article{Bizzi_et_al,
    author = {Bizzi, Emilio and Ajemian, Robert},
    title = {From motor planning to execution: a sensorimotor loop perspective},
    journal = {Journal of Neurophysiology},
    volume = {124},
    number = {6},
    pages = {1815-1823},
    year = {2020},
    doi = {10.1152/jn.00715.2019},
    note ={PMID: 33052779},
    URL = {
        https://doi.org/10.1152/jn.00715.2019
    },
    eprint = {
        https://doi.org/10.1152/jn.00715.2019
    },
    abstract = { How is an evanescent wish to move translated into a concrete action? This simple question and puzzling miracle remains a focal point of motor systems neuroscience. Where does the difficulty lie? A great deal has been known about biomechanics for quite some time. More recently, there have been significant advances in our understanding of how the spinal system is organized into modules corresponding to spinal synergies, which are fixed patterns of multimuscle recruitment. But much less is known about how the supraspinal system recruits these synergies in the correct spatiotemporal pattern to effectively control movement. We argue that what makes the problem of supraspinal control so difficult is that it emerges as a result of multiple convergent and redundant sensorimotor loops. Because these loops are convergent, multiple modes of information are mixed before being sent to the spinal system; because they are redundant, information is overlapping such that a mechanism must exist to eliminate the redundancy before the signal is sent to the spinal system. Given these complex interactions, simple correlation analyses between movement variables and neural activity are likely to render a confusing and inconsistent picture. Here, we suggest that the perspective of sensorimotor loops might help in achieving a better systems-level understanding. Furthermore, state-of-the-art techniques in neurotechnology, such as optogenetics, appear to be well suited for investigating the problem of motor control at the level of loops. }
}

@Article{Adams2013,
author={Adams, Rick A.
and Shipp, Stewart
and Friston, Karl J.},
title={Predictions not commands: active inference in the motor system},
journal={Brain Structure and Function},
year={2013},
month={05},
day={01},
volume={218},
number={3},
pages={611-643},
abstract={The descending projections from motor cortex share many features with top-down or backward connections in visual cortex; for example, corticospinal projections originate in infragranular layers, are highly divergent and (along with descending cortico-cortical projections) target cells expressing NMDA receptors. This is somewhat paradoxical because backward modulatory characteristics would not be expected of driving motor command signals. We resolve this apparent paradox using a functional characterisation of the motor system based on Helmholtz's ideas about perception; namely, that perception is inference on the causes of visual sensations. We explain behaviour in terms of inference on the causes of proprioceptive sensations. This explanation appeals to active inference, in which higher cortical levels send descending proprioceptive predictions, rather than motor commands. This process mirrors perceptual inference in sensory cortex, where descending connections convey predictions, while ascending connections convey prediction errors. The anatomical substrate of this recurrent message passing is a hierarchical system consisting of functionally asymmetric driving (ascending) and modulatory (descending) connections: an arrangement that we show is almost exactly recapitulated in the motor system, in terms of its laminar, topographic and physiological characteristics. This perspective casts classical motor reflexes as minimising prediction errors and may provide a principled explanation for why motor cortex is agranular.},
issn={1863-2661},
doi={10.1007/s00429-012-0475-5},
url={https://doi.org/10.1007/s00429-012-0475-5}
}

@article{AIF_and_Learning,
title = {Active inference and learning},
journal = {Neuroscience and Biobehavioral Reviews},
volume = {68},
pages = {862-879},
year = {2016},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2016.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0149763416301336},
author = {Karl Friston and Thomas FitzGerald and Francesco Rigoli and Philipp Schwartenbeck and John O⿿Doherty and Giovanni Pezzulo},
keywords = {Active inference, Habit learning, Bayesian inference, Goal-directed, Free energy, Information gain, Bayesian surprise, Epistemic value, Exploration, Exploitation},
abstract = {This paper offers an active inference account of choice behaviour and learning. It focuses on the distinction between goal-directed and habitual behaviour and how they contextualise each other. We show that habits emerge naturally (and autodidactically) from sequential policy optimisation when agents are equipped with state-action policies. In active inference, behaviour has explorative (epistemic) and exploitative (pragmatic) aspects that are sensitive to ambiguity and risk respectively, where epistemic (ambiguity-resolving) behaviour enables pragmatic (reward-seeking) behaviour and the subsequent emergence of habits. Although goal-directed and habitual policies are usually associated with model-based and model-free schemes, we find the more important distinction is between belief-free and belief-based schemes. The underlying (variational) belief updating provides a comprehensive (if metaphorical) process theory for several phenomena, including the transfer of dopamine responses, reversal learning, habit formation and devaluation. Finally, we show that active inference reduces to a classical (Bellman) scheme, in the absence of ambiguity.}
}

@misc{sajid2021active_bayes_optimal,
      title={Active inference, Bayesian optimal design, and expected utility}, 
      author={Noor Sajid and Lancelot Da Costa and Thomas Parr and Karl Friston},
      year={2021},
      eprint={2110.04074},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{aif_process_theory,
    author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
    title = "{Active Inference: A Process Theory}",
    journal = {Neural Computation},
    volume = {29},
    number = {1},
    pages = {1-49},
    year = {2017},
    month = {01},
    abstract = "{This article describes a process theory based on active inference and belief propagation. Starting from the premise that all neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence—or minimizing variational free energy—we ask whether neuronal responses can be described as a gradient descent on variational free energy. Using a standard (Markov decision process) generative model, we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well-characterized neuronal phenomena. These include repetition suppression, mismatch negativity, violation responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-to-bound dynamics, and transfer of dopamine responses. Furthermore, the (approximately Bayes’ optimal) behavior prescribed by these dynamics has a degree of face validity, providing a formal explanation for reward seeking, context learning, and epistemic foraging. Technically, the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton’s principle of least action.}",
    issn = {0899-7667},
    doi = {10.1162/NECO_a_00912},
    url = {https://doi.org/10.1162/NECO\_a\_00912},
    eprint = {https://direct.mit.edu/neco/article-pdf/29/1/1/984132/neco\_a\_00912.pdf},
}


@article{van_de_Laar_2016_HA,
    doi = {10.1109/taslp.2016.2599275},
  
    url = {https://doi.org/10.1109%2Ftaslp.2016.2599275},
  
    year = 2016,
    month = {11},
  
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
    volume = {24},
  
    number = {11},
  
    pages = {2200--2213},
  
    author = {Thijs van de Laar and Bert de Vries},
  
    title = {A Probabilistic Modeling Approach to Hearing Loss Compensation},
  
    journal = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing}
}

@article{AIF-A-Process-Theory,
    author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
    title = "{Active Inference: A Process Theory}",
    journal = {Neural Computation},
    volume = {29},
    number = {1},
    pages = {1-49},
    year = {2017},
    month = {01},
    abstract = "{This article describes a process theory based on active inference and belief propagation. Starting from the premise that all neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence—or minimizing variational free energy—we ask whether neuronal responses can be described as a gradient descent on variational free energy. Using a standard (Markov decision process) generative model, we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well-characterized neuronal phenomena. These include repetition suppression, mismatch negativity, violation responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-to-bound dynamics, and transfer of dopamine responses. Furthermore, the (approximately Bayes’ optimal) behavior prescribed by these dynamics has a degree of face validity, providing a formal explanation for reward seeking, context learning, and epistemic foraging. Technically, the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton’s principle of least action.}",
    issn = {0899-7667},
    doi = {10.1162/NECO_a_00912},
    url = {https://doi.org/10.1162/NECO\_a\_00912},
    eprint = {https://direct.mit.edu/neco/article-pdf/29/1/1/984132/neco\_a\_00912.pdf},
}

@article{AIF-Demystified,
    author = {Sajid, Noor and Ball, Philip J. and Parr, Thomas and Friston, Karl J.},
    title = "{Active Inference: Demystified and Compared}",
    journal = {Neural Computation},
    volume = {33},
    number = {3},
    pages = {674-712},
    year = {2021},
    month = {03},
    abstract = "{Active inference is a first principle account of how autonomous agents operate in dynamic, nonstationary environments. This problem is also considered in reinforcement learning, but limited work exists on comparing the two approaches on the same discrete-state environments. In this letter, we provide (1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in reinforcement learning, and (2) an explicit discrete-state comparison between active inference and reinforcement learning on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of reinforcement learning. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration—and account for uncertainty about their
          environment—in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in reinforcement learning is removed in active inference, where reward can simply be treated as another observation we have a preference over; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based reinforcement learning agents and by placing zero prior preferences over rewards and learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings (e.g., robotic arm movement, Atari games) if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete
          state-space and time formulation and demonstrate these behaviors in a OpenAI gym environment, alongside reinforcement learning agents.}",
    issn = {0899-7667},
    doi = {10.1162/neco_a_01357},
    url = {https://doi.org/10.1162/neco\_a\_01357},
    eprint = {https://direct.mit.edu/neco/article-pdf/33/3/674/1889396/neco\_a\_01357.pdf},
}

@misc{BayesThermoRepo,
  author = {Andre Van Schaik},
  title = {{Active-Inference}},
  year = {2018},
  howpublished = {\url{https://github.com/vschaik/Active-Inference}},
  note = {Simple Active Inference Implementation}
}

@article {Computational-Mech-Curiosity-Goal-Exploration,
    article_type = {journal},
    title = {Computational mechanisms of curiosity and goal-directed exploration},
    author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas HB and Kronbichler, Martin and Friston, Karl J},
    editor = {Frank, Michael J},
    volume = 8,
    year = 2019,
    month = {05},
    pub_date = {2019-05-10},
    pages = {e41703},
    citation = {eLife 2019;8:e41703},
    doi = {10.7554/eLife.41703},
    url = {https://doi.org/10.7554/eLife.41703},
    abstract = {Successful behaviour depends on the right balance between maximising reward and soliciting information about the world. Here, we show how different types of information-gain emerge when casting behaviour as surprise minimisation. We present two distinct mechanisms for goal-directed exploration that express separable profiles of active sampling to reduce uncertainty. ‘Hidden state’ exploration motivates agents to sample unambiguous observations to accurately infer the (hidden) state of the world. Conversely, ‘model parameter’ exploration, compels agents to sample outcomes associated with high uncertainty, if they are informative for their representation of the task structure. We illustrate the emergence of these types of information-gain, termed active inference and active learning, and show how these forms of exploration induce distinct patterns of ‘Bayes-optimal’ behaviour. Our findings provide a computational framework for understanding how distinct levels of uncertainty systematically affect the exploration-exploitation trade-off in decision-making.},
    keywords = {exploration, exploitation, active learning, active inference, curiosity, intrinsic motivation},
    journal = {eLife},
    issn = {2050-084X},
    publisher = {eLife Sciences Publications, Ltd},
}

@article{AIF_Epis_Val,
    author = {Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald and Giovanni Pezzulo},
    title = {Active inference and epistemic value},
    journal = {Cognitive Neuroscience},
    volume = {6},
    number = {4},
    pages = {187-214},
    year = {2015},
    publisher = {Routledge},
    doi = {10.1080/17588928.2015.1020053},
    note ={PMID: 25689102},
    URL = {https://doi.org/10.1080/17588928.2015.1020053},
    eprint = {https://doi.org/10.1080/17588928.2015.1020053}
}

@Article{Free-Energy-Users-Guide,
    author={Mann, Stephen Francis
    and Pain, Ross
    and Kirchhoff, Michael D.},
    title={Free energy: a user's guide},
    journal={Biology {\&} Philosophy},
    year={2022},
    month={07},
    day={20},
    volume={37},
    number={4},
    pages={33},
    abstract={Over the last fifteen years, an ambitious explanatory framework has been proposed to unify explanations across biology and cognitive science. Active inference, whose most famous tenet is the free energy principle, has inspired excitement and confusion in equal measure. Here, we lay the ground for proper critical analysis of active inference, in three ways. First, we give simplified versions of its core mathematical models. Second, we outline the historical development of active inference and its relationship to other theoretical approaches. Third, we describe three different kinds of claim---labelled mathematical, empirical and general---routinely made by proponents of the framework, and suggest dialectical links between them. Overall, we aim to increase philosophical understanding of active inference so that it may be more readily evaluated. This paper is the Introduction to the Topical Collection ``The Free Energy Principle: From Biology to Cognition''.},
    issn={1572-8404},
    doi={10.1007/s10539-022-09864-z},
    url={https://doi.org/10.1007/s10539-022-09864-z}
}

@online{FEP-Rough-Guide-Brain,
    author        = {Karl Friston},
    title         = {The free-energy principle: a rough guide to the brain?},
    year          = {2009},
    url           = {https://doi.org/10.1016/j.tics.2009.04.005}
}

@Article{FEP-Unified-Brain-Theory,
    author={Friston, Karl},
    title={The free-energy principle: a unified brain theory?},
    journal={Nature Reviews Neuroscience},
    year={2010},
    month={02},
    day={01},
    volume={11},
    number={2},
    pages={127-138},
    abstract={Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
    issn={1471-0048},
    doi={10.1038/nrn2787},
    url={https://doi.org/10.1038/nrn2787}
}


@article{A_FEP_For_The_Brain,
    title = {A free energy principle for the brain},
    journal = {Journal of Physiology-Paris},
    volume = {100},
    number = {1},
    pages = {70-87},
    year = {2006},
    note = {Theoretical and Computational Neuroscience: Understanding Brain Functions},
    issn = {0928-4257},
    doi = {https://doi.org/10.1016/j.jphysparis.2006.10.001},
    url = {https://www.sciencedirect.com/science/article/pii/S092842570600060X},
    author = {Karl Friston and James Kilner and Lee Harrison},
    keywords = {Variational Bayes, Free energy, Inference, Perception, Action, Learning, Attention, Selection, Hierarchical},
    abstract = {By formulating Helmholtz’s ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses. In this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy principle. The free energy considered here measures the difference between the probability distribution of environmental quantities that act on the system and an arbitrary distribution encoded by its configuration. The system can minimise free energy by changing its configuration to affect the way it samples the environment or change the distribution it encodes. These changes correspond to action and perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment assumes that the system’s state and structure encode an implicit and probabilistic model of the environment. We will look at the models entailed by the brain and how minimisation of its free energy can explain its dynamics and structure.}
}

@book{Doya_Bayes,
    author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh P.N.},
    title = "{Bayesian Brain: Probabilistic Approaches to Neural Coding }",
    publisher = {The MIT Press},
    year = {2006},
    month = {12},
    abstract = "{Experimental and theoretical neuroscientists use Bayesian approaches to analyze the brain mechanisms of perception, decision-making, and motor control.A Bayesian approach can contribute to an understanding of the brain on multiple levels, by giving normative predictions about how an ideal sensory system should combine prior knowledge and observation, by providing mechanistic interpretation of the dynamic functioning of the brain circuit, and by suggesting optimal ways of deciphering experimental data. Bayesian Brain brings together contributions from both experimental and theoretical neuroscientists that examine the brain mechanisms of perception, decision making, and motor control according to the concepts of Bayesian estimation.After an overview of the mathematical concepts, including Bayes' theorem, that are basic to understanding the approaches discussed, contributors discuss how Bayesian concepts can be used for interpretation of such neurobiological data as neural spikes and functional brain imaging. Next, contributors examine the modeling of sensory processing, including the neural coding of information about the outside world. Finally, contributors explore dynamic processes for proper behaviors, including the mathematics of the speed and accuracy of perceptual decisions and neural models of belief propagation.}",
    isbn = {9780262294188},
    doi = {10.7551/mitpress/9780262042383.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262042383.001.0001},
}

@article{DELANGE2018,
title = {How Do Expectations Shape Perception?},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {9},
pages = {764-779},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318301396},
author = {Floris P. {de Lange} and Micha Heilbron and Peter Kok},
keywords = {prediction, perception, sensory processing, Bayesian inference, predictive coding, perceptual inference},
abstract = {Perception and perceptual decision-making are strongly facilitated by prior knowledge about the probabilistic structure of the world. While the computational benefits of using prior expectation in perception are clear, there are myriad ways in which this computation can be realized. We review here recent advances in our understanding of the neural sources and targets of expectations in perception. Furthermore, we discuss Bayesian theories of perception that prescribe how an agent should integrate prior knowledge and sensory information, and investigate how current and future empirical data can inform and constrain computational frameworks that implement such probabilistic integration in perception.}
}

@article{Clark2013,
  author    = {Andy Clark},
  title     = {Whatever Next? Predictive Brains, Situated Agents, and the Future of Cognitive Science},
  journal   = {Behavioral and Brain Sciences},
  volume    = {36},
  number    = {3},
  pages     = {181--204},
  year      = {2013},
  doi       = {10.1017/S0140525X12000477},
  url       = {https://doi.org/10.1017/S0140525X12000477},
  pmid      = {23663408},
}


@article{Life-As-We-Know-It,
    author = {Friston, Karl },
    title = {Life as we know it},
    journal = {Journal of The Royal Society Interface},
    volume = {10},
    number = {86},
    pages = {20130475},
    year = {2013},
    doi = {10.1098/rsif.2013.0475},

    URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2013.0475},
    eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2013.0475}
    ,
        abstract = { This paper presents a heuristic proof (and simulations of a primordial soup) suggesting that life—or biological self-organization—is an inevitable and emergent property of any (ergodic) random dynamical system that possesses a Markov blanket. This conclusion is based on the following arguments: if the coupling among an ensemble of dynamical systems is mediated by short-range forces, then the states of remote systems must be conditionally independent. These independencies induce a Markov blanket that separates internal and external states in a statistical sense. The existence of a Markov blanket means that internal states will appear to minimize a free energy functional of the states of their Markov blanket. Crucially, this is the same quantity that is optimized in Bayesian inference. Therefore, the internal states (and their blanket) will appear to engage in active Bayesian inference. In other words, they will appear to model—and act on—their world to preserve their functional and structural integrity, leading to homoeostasis and a simple form of autopoiesis. }
}

@article{Tutorial-FEP-Modelling-Perception-Action,
    title = {A tutorial on the free-energy framework for modelling perception and learning},
    journal = {Journal of Mathematical Psychology},
    volume = {76},
    pages = {198-211},
    year = {2017},
    note = {Model-based Cognitive Neuroscience},
    issn = {0022-2496},
    doi = {https://doi.org/10.1016/j.jmp.2015.11.003},
    url = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
    author = {Rafal Bogacz},
    abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.}
}

@Article{Rao1999,
author={Rao, Rajesh P. N.
and Ballard, Dana H.},
title={Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
journal={Nature Neuroscience},
year={1999},
month={01},
day={01},
volume={2},
number={1},
pages={79-87},
abstract={We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
issn={1546-1726},
doi={10.1038/4580},
url={https://doi.org/10.1038/4580}
}


@misc{gymnasium_2023,
    title = {Gymnasium},
    url = {https://zenodo.org/record/8127025},
    abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
    urldate = {2023-07-08},
    publisher = {Zenodo},
    author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and Cola, Gianluca de and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
    month = mar,
    year = {2023},
    doi = {10.5281/zenodo.8127026},
}

@book{Dynamics_in_Action,
    author = {Juarrero, Alicia},
    title = "{Dynamics in Action: Intentional Behavior as a Complex System}",
    publisher = {The MIT Press},
    year = {1999},
    month = {11},
    abstract = "{What is the difference between a wink and a blink? The answer is important not only to philosophers of mind, for significant moral and legal consequences rest on the distinction between voluntary and involuntary behavior. However, "action theory"—the branch of philosophy that has traditionally articulated the boundaries between action and non-action, and between voluntary and involuntary behavior—has been unable to account for the difference.Alicia Juarrero argues that a mistaken, 350-year-old model of cause and explanation—one that takes all causes to be of the push-pull, efficient cause sort, and all explanation to be prooflike—underlies contemporary theories of action. Juarrero then proposes a new framework for conceptualizing causes based on complex adaptive systems. Thinking of causes as dynamical constraints makes bottom-up and top-down causal relations, including those involving intentional causes, suddenly tractable. A different logic for explaining actions—as historical narrative, not inference—follows if one adopts this novel approach to long-standing questions of action and responsibility.}",
    isbn = {9780262276542},
    doi = {10.7551/mitpress/2528.001.0001},
    url = {https://doi.org/10.7551/mitpress/2528.001.0001},
}


@book{gibson2014ecological,
  author = {Gibson, James J.},
  year = {2014},
  title = {The Ecological Approach to Visual Perception: Classic Edition},
  edition = {1st},
  publisher = {Psychology Press},
  doi = {10.4324/9781315740218}
}

@article{Spearman,
 ISSN = {00029556},
 URL = {http://www.jstor.org/stable/1412107},
 author = {C. Spearman},
 journal = {The American Journal of Psychology},
 number = {2},
 pages = {201--292},
 publisher = {University of Illinois Press},
 title = {"General Intelligence," Objectively Determined and Measured},
 urldate = {2023-08-08},
 volume = {15},
 year = {1904}
}

@article{FEP-Mathematical-Review,
    title = {The free energy principle for action and perception: A mathematical review},
    journal = {Journal of Mathematical Psychology},
    volume = {81},
    pages = {55-79},
    year = {2017},
    issn = {0022-2496},
    doi = {https://doi.org/10.1016/j.jmp.2017.09.004},
    url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
    author = {Christopher L. Buckley and Chang Sub Kim and Simon McGregor and Anil K. Seth},
    keywords = {Free energy principle, Perception, Action, Inference, Bayesian brain, Agent-based model},
    abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.}
}

@article{Emperical-Eval-AIF-Multi-Arm-Bandits,
    title = {An empirical evaluation of active inference in multi-armed bandits},
    journal = {Neural Networks},
    volume = {144},
    pages = {229-246},
    year = {2021},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/j.neunet.2021.08.018},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608021003233},
    author = {Dimitrije Marković and Hrvoje Stojić and Sarah Schwöbel and Stefan J. Kiebel},
    keywords = {Decision making, Bayesian inference, Multi-armed bandits, Active inference, Upper confidence bound, Thompson sampling},
    abstract = {A key feature of sequential decision making under uncertainty is a need to balance between exploiting—choosing the best action according to the current knowledge, and exploring—obtaining information about values of other actions. The multi-armed bandit problem, a classical task that captures this trade-off, served as a vehicle in machine learning for developing bandit algorithms that proved to be useful in numerous industrial applications. The active inference framework, an approach to sequential decision making recently developed in neuroscience for understanding human and animal behaviour, is distinguished by its sophisticated strategy for resolving the exploration–exploitation trade-off. This makes active inference an exciting alternative to already established bandit algorithms. Here we derive an efficient and scalable approximate active inference algorithm and compare it to two state-of-the-art bandit algorithms: Bayesian upper confidence bound and optimistic Thompson sampling. This comparison is done on two types of bandit problems: a stationary and a dynamic switching bandit. Our empirical evaluation shows that the active inference algorithm does not produce efficient long-term behaviour in stationary bandits. However, in the more challenging switching bandit problem active inference performs substantially better than the two state-of-the-art bandit algorithms. The results open exciting venues for further research in theoretical and applied machine learning, as well as lend additional credibility to active inference as a general framework for studying human and animal behaviour.}
}

@article{WOLPERT1997209,
title = {Computational approaches to motor control},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {6},
pages = {209-216},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01070-X},
url = {https://www.sciencedirect.com/science/article/pii/S136466139701070X},
author = {Daniel M. Wolpert},
abstract = {This review will focus on four areas of motor control which have recently been enriched both by neural network and control system models: motor planning, motor prediction, state estimation and motor learning. We will review the computational foundations of each of these concepts and present specific models which have been tested by psychophysical experiments. We will cover the topics of optimal control for motor planning, forward models for motor prediction, observer models of state estimation arid modular decomposition in motor learning. The aim of this review is to demonstrate how computational approaches, as well as proposing specific models, provide a theoretical framework to formalize the issues in motor control.}
}

@book{Hohwy,
    author = {Hohwy, Jakob},
    title = "{The Predictive Mind}",
    publisher = {Oxford University Press},
    year = {2013},
    month = {11},
    abstract = "{A new theory is taking hold in neuroscience. The theory is increasingly being used to interpret and drive experimental and theoretical studies, and it is finding its way into many other domains of research on the mind. It is the theory that the brain is a sophisticated hypothesis-testing mechanism, which is constantly involved in minimizing the error of its predictions about the sensory input it receives from the world. This mechanism is meant to explain perception and action and everything mental in between. It is an attractive theory because powerful theoretical arguments support it. It is also attractive because more and more empirical evidence is beginning to point in its favour. It has enormous unifying power and yet it can explain in detail too. This book explores this theory. It explains how the theory works and how it applies; it sets out why the theory is attractive; and it shows why and how the central ideas behind the theory profoundly change how we should conceive of perception, action, attention, and other central aspects of the mind. The central argument of the book is that the simple idea of prediction error minimization offers a surprisingly good, explanatory fit with our actual perceptual phenomenology, and that it throws new light on core, intriguing aspects of the nature of mind.}",
    isbn = {9780199682737},
    doi = {10.1093/acprof:oso/9780199682737.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780199682737.001.0001},
}

@article{Action-Behaviour-FE,
    author = {Friston, K  and Daunizeau, J  and Kilner, J  and Kiebel, S},
    journal = {Biological cybernetics},
    title = {Action and behavior: a free-energy formulation},
    year = {2010},
    volume = {102},
    number = {3},
    pages = {227-260},
    doi = {10.1007/s00422-010-0364-z},
    url = {https://doi.org/10.1007/s00422-010-0364-z}
}

@article{Codes-on-Graphs,
  author={Forney, G.D.},
  journal={IEEE Transactions on Information Theory}, 
  title={Codes on graphs: normal realizations}, 
  year={2001},
  volume={47},
  number={2},
  pages={520-548},
  doi={10.1109/18.910573}
}

@article{Markov-Blankets-Life,
    author = {Kirchhoff, Michael  and Parr, Thomas  and Palacios, Ensor  and Friston, Karl  and Kiverstein, Julian },
    title = {The Markov blankets of life: autonomy, active inference and the free energy principle},
    journal = {Journal of The Royal Society Interface},
    volume = {15},
    number = {138},
    pages = {20170792},
    year = {2018},
    doi = {10.1098/rsif.2017.0792},

    URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2017.0792},
    eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0792}
    ,
    abstract = { This work addresses the autonomous organization of biological systems. It does so by considering the boundaries of biological systems, from individual cells to Home sapiens, in terms of the presence of Markov blankets under the active inference scheme—a corollary of the free energy principle. A Markov blanket defines the boundaries of a system in a statistical sense. Here we consider how a collective of Markov blankets can self-assemble into a global system that itself has a Markov blanket; thereby providing an illustration of how autonomous systems can be understood as having layers of nested and self-sustaining boundaries. This allows us to show that: (i) any living system is a Markov blanketed system and (ii) the boundaries of such systems need not be co-extensive with the biophysical boundaries of a living organism. In other words, autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me, and all the way out to include elements of the local environment. }
}

@Inbook{View-of-EM-Algorithm,
    author="Neal, Radford M.
    and Hinton, Geoffrey E.",
    editor="Jordan, Michael I.",
    title="A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants",
    bookTitle="Learning in Graphical Models",
    year="1998",
    publisher="Springer Netherlands",
    address="Dordrecht",
    pages="355--368",
    abstract="The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.",
    isbn="978-94-011-5014-9",
    doi="10.1007/978-94-011-5014-9_12",
    url="https://doi.org/10.1007/978-94-011-5014-9_12"
}

@ARTICLE{Attention-UUncertainty-Free-Energy,
    AUTHOR={Feldman, Harriet and Friston, Karl},    
    TITLE={Attention, Uncertainty, and Free-Energy},      
    JOURNAL={Frontiers in Human Neuroscience},      
    VOLUME={4},           
    YEAR={2010},        
    URL={https://www.frontiersin.org/articles/10.3389/fnhum.2010.00215},       
    DOI={10.3389/fnhum.2010.00215},      
    ISSN={1662-5161},   
    ABSTRACT={We suggested recently that attention can be understood as inferring the level of uncertainty or precision during hierarchical perception. In this paper, we try to substantiate this claim using neuronal simulations of directed spatial attention and biased competition. These simulations assume that neuronal activity encodes a probabilistic representation of the world that optimizes free-energy in a Bayesian fashion. Because free-energy bounds surprise or the (negative) log-evidence for internal models of the world, this optimization can be regarded as evidence accumulation or (generalized) predictive coding. Crucially, both predictions about the state of the world generating sensory data and the precision of those data have to be optimized. Here, we show that if the precision depends on the states, one can explain many aspects of attention. We illustrate this in the context of the Posner paradigm, using the simulations to generate both psychophysical and electrophysiological responses. These simulated responses are consistent with attentional bias or gating, competition for attentional resources, attentional capture and associated speed-accuracy trade-offs. Furthermore, if we present both attended and non-attended stimuli simultaneously, biased competition for neuronal representation emerges as a principled and straightforward property of Bayes-optimal perception.}
}

@article{The-Bayesian-Brain,
    title = {The Bayesian brain: the role of uncertainty in neural coding and computation},
    journal = {Trends in Neurosciences},
    volume = {27},
    number = {12},
    pages = {712-719},
    year = {2004},
    issn = {0166-2236},
    doi = {https://doi.org/10.1016/j.tins.2004.10.007},
    url = {https://www.sciencedirect.com/science/article/pii/S0166223604003352},
    author = {David C. Knill and Alexandre Pouget},
    abstract = {To use sensory information efficiently to make judgments and guide action in the world, the brain must represent and use information about uncertainty in its computations for perception and action. Bayesian methods have proven successful in building computational theories for perception and sensorimotor control, and psychophysics is providing a growing body of evidence that human perceptual computations are ‘Bayes' optimal’. This leads to the ‘Bayesian coding hypothesis’: that the brain represents sensory information probabilistically, in the form of probability distributions. Several computational schemes have recently been proposed for how this might be achieved in populations of neurons. Neurophysiological data on the hypothesis, however, is almost non-existent. A major challenge for neuroscientists is to test these ideas experimentally, and so determine whether and how neurons code information about sensory uncertainty.}
}

@article{Demystifying-AIF,
  author    = {Noor Sajid and
               Philip J. Ball and
               Karl J. Friston},
  title     = {Demystifying active inference},
  journal   = {CoRR},
  volume    = {abs/1909.10863},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.10863},
  eprinttype = {arXiv},
  eprint    = {1909.10863},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-10863.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Article{Generalised-Free-Energy-AIF,
    author={Parr, Thomas
    and Friston, Karl J.},
    title={Generalised free energy and active inference},
    journal={Biological Cybernetics},
    year={2019},
    month={12},
    day={01},
    volume={113},
    number={5},
    pages={495-513},
    abstract={Active inference is an approach to understanding behaviour that rests upon the idea that the brain uses an internal generative model to predict incoming sensory data. The fit between this model and data may be improved in two ways. The brain could optimise probabilistic beliefs about the variables in the generative model (i.e. perceptual inference). Alternatively, by acting on the world, it could change the sensory data, such that they are more consistent with the model. This implies a common objective function (variational free energy) for action and perception that scores the fit between an internal model and the world. We compare two free energy functionals for active inference in the framework of Markov decision processes. One of these is a functional of beliefs (i.e. probability distributions) about states and policies, but a function of observations, while the second is a functional of beliefs about all three. In the former (expected free energy), prior beliefs about outcomes are not part of the generative model (because they are absorbed into the prior over policies). Conversely, in the second (generalised free energy), priors over outcomes become an explicit component of the generative model. When using the free energy function, which is blind to future observations, we equip the generative model with a prior over policies that ensure preferred (i.e. priors over) outcomes are realised. In other words, if we expect to encounter a particular kind of outcome, this lends plausibility to those policies for which this outcome is a consequence. In addition, this formulation ensures that selected policies minimise uncertainty about future outcomes by minimising the free energy expected in the future. When using the free energy functional---that effectively treats future observations as hidden states---we show that policies are inferred or selected that realise prior preferences by minimising the free energy of future expectations. Interestingly, the form of posterior beliefs about policies (and associated belief updating) turns out to be identical under both formulations, but the quantities used to compute them are not.},
    issn={1432-0770},
    doi={10.1007/s00422-019-00805-w},
    url={https://doi.org/10.1007/s00422-019-00805-w}
}

@ARTICLE{MCTS_Survey,
  author={Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in Games}, 
  title={A Survey of Monte Carlo Tree Search Methods}, 
  year={2012},
  volume={4},
  number={1},
  pages={1-43},
  doi={10.1109/TCIAIG.2012.2186810}
}

@article{MCTS_New_Frame,
  title={Monte-Carlo Tree Search: A New Framework for Game AI},
  author={Guillaume Chaslot and Sander C. J. Bakkes and Istv{\'a}n Szita and Pieter Spronck},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
  year={2008},
  url={https://api.semanticscholar.org/CorpusID:12448260}
}

@Article{Planning-and-Nav-as-AIF,
    author={Kaplan, Raphael
    and Friston, Karl J.},
    title={Planning and navigation as active inference},
    journal={Biological Cybernetics},
    year={2018},
    month={08},
    day={01},
    volume={112},
    number={4},
    pages={323-343},
    abstract={This paper introduces an active inference formulation of planning and navigation. It illustrates how the exploitation--exploration dilemma is dissolved by acting to minimise uncertainty (i.e. expected surprise or free energy). We use simulations of a maze problem to illustrate how agents can solve quite complicated problems using context sensitive prior preferences to form subgoals. Our focus is on how epistemic behaviour---driven by novelty and the imperative to reduce uncertainty about the world---contextualises pragmatic or goal-directed behaviour. Using simulations, we illustrate the underlying process theory with synthetic behavioural and electrophysiological responses during exploration of a maze and subsequent navigation to a target location. An interesting phenomenon that emerged from the simulations was a putative distinction between `place cells'---that fire when a subgoal is reached---and `path cells'---that fire until a subgoal is reached.},
    issn={1432-0770},
    doi={10.1007/s00422-018-0753-2},
    url={https://doi.org/10.1007/s00422-018-0753-2}
}

@Article{Mann2022,
author={Mann, Stephen Francis
and Pain, Ross
and Kirchhoff, Michael D.},
title={Free energy: a user's guide},
journal={Biology {\&} Philosophy},
year={2022},
month={07},
day={20},
volume={37},
number={4},
pages={33},
abstract={Over the last fifteen years, an ambitious explanatory framework has been proposed to unify explanations across biology and cognitive science. Active inference, whose most famous tenet is the free energy principle, has inspired excitement and confusion in equal measure. Here, we lay the ground for proper critical analysis of active inference, in three ways. First, we give simplified versions of its core mathematical models. Second, we outline the historical development of active inference and its relationship to other theoretical approaches. Third, we describe three different kinds of claim---labelled mathematical, empirical and general---routinely made by proponents of the framework, and suggest dialectical links between them. Overall, we aim to increase philosophical understanding of active inference so that it may be more readily evaluated. This paper is the Introduction to the Topical Collection ``The Free Energy Principle: From Biology to Cognition''.},
issn={1572-8404},
doi={10.1007/s10539-022-09864-z},
url={https://doi.org/10.1007/s10539-022-09864-z}
}



@article{AIF-Discrete-Action-Spaces-Synthesis,
    title = {Active inference on discrete state-spaces: A synthesis},
    journal = {Journal of Mathematical Psychology},
    volume = {99},
    pages = {102447},
    year = {2020},
    issn = {0022-2496},
    doi = {https://doi.org/10.1016/j.jmp.2020.102447},
    url = {https://www.sciencedirect.com/science/article/pii/S0022249620300857},
    author = {Lancelot {Da Costa} and Thomas Parr and Noor Sajid and Sebastijan Veselic and Victorita Neacsu and Karl Friston},
    keywords = {Active inference, Free energy principle, Process theory, Variational Bayesian inference, Markov decision process, Mathematical review},
    abstract = {Active inference is a normative principle underwriting perception, action, planning, decision-making and learning in biological or artificial agents. From its inception, its associated process theory has grown to incorporate complex generative models, enabling simulation of a wide range of complex behaviours. Due to successive developments in active inference, it is often difficult to see how its underlying principle relates to process theories and practical implementation. In this paper, we try to bridge this gap by providing a complete mathematical synthesis of active inference on discrete state-space models. This technical summary provides an overview of the theory, derives neuronal dynamics from first principles and relates this dynamics to biological processes. Furthermore, this paper provides a fundamental building block needed to understand active inference for mixed generative models; allowing continuous sensations to inform discrete representations. This paper may be used as follows: to guide research towards outstanding challenges, a practical guide on how to implement active inference to simulate experimental behaviour, or a pointer towards various in-silico neurophysiological responses that may be used to make empirical predictions.}
}

@Article{RL-Real-World-Challenges,
    author={Dulac-Arnold, Gabriel
    and Levine, Nir
    and Mankowitz, Daniel J.
    and Li, Jerry
    and Paduraru, Cosmin
    and Gowal, Sven
    and Hester, Todd},
    title={Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
    journal={Machine Learning},
    year={2021},
    month={09},
    day={01},
    volume={110},
    number={9},
    pages={2419-2468},
    abstract={Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
    issn={1573-0565},
    doi={10.1007/s10994-021-05961-4},
    url={https://doi.org/10.1007/s10994-021-05961-4}
}


@article{RR-Emerging,
    author = {Vervaeke, John and Lillicrap, Timothy P. and Richards, Blake A.},
    title = "{Relevance Realization and the Emerging Framework in Cognitive Science}",
    journal = {Journal of Logic and Computation},
    volume = {22},
    number = {1},
    pages = {79-99},
    year = {2009},
    month = {10},
    abstract = "{We argue that an explanation of relevance realization is a pervasive problem within cognitive science, and that it is becoming the criterion of the cognitive in terms of which a new framework for doing cognitive science is emerging. We articulate that framework and then make use of it to provide the beginnings of a theory of relevance realization that incorporates many existing insights implicit within the contributing disciplines of cognitive science. We also introduce some theoretical and potentially technical innovations motivated by the articulation of those insights. Finally, we show how the explication of the framework and development of the theory help to clear up some important incompleteness and confusions within both Montague's work and Sperber and Wilson's theory of relevance.}",
    issn = {0955-792X},
    doi = {10.1093/logcom/exp067},
    url = {https://doi.org/10.1093/logcom/exp067},
    eprint = {https://academic.oup.com/logcom/article-pdf/22/1/79/3262477/exp067.pdf},
}

@Article{MuZero,
    author={Schrittwieser, Julian
    and Antonoglou, Ioannis
    and Hubert, Thomas
    and Simonyan, Karen
    and Sifre, Laurent
    and Schmitt, Simon
    and Guez, Arthur
    and Lockhart, Edward
    and Hassabis, Demis
    and Graepel, Thore
    and Lillicrap, Timothy
    and Silver, David},
    title={Mastering Atari, Go, chess and shogi by planning with a learned model},
    journal={Nature},
    year={2020},
    month={12},
    day={01},
    volume={588},
    number={7839},
    pages={604-609},
    abstract={Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
    issn={1476-4687},
    doi={10.1038/s41586-020-03051-4},
    url={https://doi.org/10.1038/s41586-020-03051-4}
}


@article{Mastering-Go-Without-Human-Knowledge,
    author={Silver, David
    and Schrittwieser, Julian
    and Simonyan, Karen
    and Antonoglou, Ioannis
    and Huang, Aja
    and Guez, Arthur
    and Hubert, Thomas
    and Baker, Lucas
    and Lai, Matthew
    and Bolton, Adrian
    and Chen, Yutian
    and Lillicrap, Timothy
    and Hui, Fan
    and Sifre, Laurent
    and van den Driessche, George
    and Graepel, Thore
    and Hassabis, Demis},
    title={Mastering the game of Go without human knowledge},
    journal={Nature},
    year={2017},
    month={10},
    volume={550},
    number={7676},
    pages={354-359},
    abstract={A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
    issn={1476-4687},
    doi={10.1038/nature24270},
    url={https://doi.org/10.1038/nature24270}
}

@Article{Silver2016,
    author={Silver, David
    and Huang, Aja
    and Maddison, Chris J.
    and Guez, Arthur
    and Sifre, Laurent
    and van den Driessche, George
    and Schrittwieser, Julian
    and Antonoglou, Ioannis
    and Panneershelvam, Veda
    and Lanctot, Marc
    and Dieleman, Sander
    and Grewe, Dominik
    and Nham, John
    and Kalchbrenner, Nal
    and Sutskever, Ilya
    and Lillicrap, Timothy
    and Leach, Madeleine
    and Kavukcuoglu, Koray
    and Graepel, Thore
    and Hassabis, Demis},
    title={Mastering the game of Go with deep neural networks and tree search},
    journal={Nature},
    year={2016},
    month={01},
    day={01},
    volume={529},
    number={7587},
    pages={484-489},
    abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
    issn={1476-4687},
    doi={10.1038/nature16961},
    url={https://doi.org/10.1038/nature16961}
}

@misc{Dream-to-Control,
      title={Dream to Control: Learning Behaviors by Latent Imagination}, 
      author={Danijar Hafner and Timothy Lillicrap and Jimmy Ba and Mohammad Norouzi},
      year={2020},
      eprint={1912.01603},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hafner2019learning,
      title={Learning Latent Dynamics for Planning from Pixels}, 
      author={Danijar Hafner and Timothy Lillicrap and Ian Fischer and Ruben Villegas and David Ha and Honglak Lee and James Davidson},
      year={2019},
      eprint={1811.04551},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{yin2023sample,
      title={Sample Efficient Deep Reinforcement Learning via Local Planning}, 
      author={Dong Yin and Sridhar Thiagarajan and Nevena Lazic and Nived Rajaraman and Botao Hao and Csaba Szepesvari},
      year={2023},
      eprint={2301.12579},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kotb2023sampleefficient,
      title={Sample-efficient Real-time Planning with Curiosity Cross-Entropy Method and Contrastive Learning}, 
      author={Mostafa Kotb and Cornelius Weber and Stefan Wermter},
      year={2023},
      eprint={2303.03787},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{curi2020efficient,
      title={Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning}, 
      author={Sebastian Curi and Felix Berkenkamp and Andreas Krause},
      year={2020},
      eprint={2006.08684},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Step-by-Step-Tutorial-AIF-Empirical-Data,
    title = {A step-by-step tutorial on active inference and its application to empirical data},
    journal = {Journal of Mathematical Psychology},
    volume = {107},
    pages = {102632},
    year = {2022},
    issn = {0022-2496},
    doi = {https://doi.org/10.1016/j.jmp.2021.102632},
    url = {https://www.sciencedirect.com/science/article/pii/S0022249621000973},
    author = {Ryan Smith and Karl J. Friston and Christopher J. Whyte},
    keywords = {Active inference, Computational neuroscience, Bayesian inference, Learning, Decision-making, Machine learning},
    abstract = {The active inference framework, and in particular its recent formulation as a partially observable Markov decision process (POMDP), has gained increasing popularity in recent years as a useful approach for modeling neurocognitive processes. This framework is highly general and flexible in its ability to be customized to model any cognitive process, as well as simulate predicted neuronal responses based on its accompanying neural process theory. It also affords both simulation experiments for proof of principle and behavioral modeling for empirical studies. However, there are limited resources that explain how to build and run these models in practice, which limits their widespread use. Most introductions assume a technical background in programming, mathematics, and machine learning. In this paper we offer a step-by-step tutorial on how to build POMDPs, run simulations using standard MATLAB routines, and fit these models to empirical data. We assume a minimal background in programming and mathematics, thoroughly explain all equations, and provide exemplar scripts that can be customized for both theoretical and empirical studies. Our goal is to provide the reader with the requisite background knowledge and practical tools to apply active inference to their own research. We also provide optional technical sections and multiple appendices, which offer the interested reader additional technical details. This tutorial should provide the reader with all the tools necessary to use these models and to follow emerging advances in active inference research.}
}

@article{Variational-Inference-Reviews,
	doi = {10.1080/01621459.2017.1285773},
	year = 2017,
	month = {04},
	publisher = {Informa {UK} Limited},
	volume = {112},
	number = {518},
	pages = {859--877},
	author = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
	title = {Variational Inference: A Review for Statisticians},
	journal = {Journal of the American Statistical Association}
}

@misc{Practical-Tutorial-Variational-Bayes,
      title={A practical tutorial on Variational Bayes}, 
      author={Minh-Ngoc Tran and Trong-Nghia Nguyen and Viet-Hung Dao},
      year={2021},
      eprint={2103.01327},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

@book{elements,
  title={Elements of Information Theory},
  author={Cover, T.M. and Thomas, J.A.},
  isbn={9781118585771},
  lccn={2005047799},
  url={https://books.google.com.au/books?id=VWq5GG6ycxMC},
  year={2012},
  publisher={Wiley}
}

@article{AIF-Curiosity-Insight,
    author = {Friston, Karl and Lin, Marco and Frith, Chris and Pezzulo, Giovanni and Hobson, J. and Ondobaka, Sasha},
    year = {2017},
    month = {08},
    pages = {1-51},
    title = {Active Inference, Curiosity and Insight},
    volume = {29},
    journal = {Neural Computation},
    doi = {10.1162/neco_a_00999}
}

@book{Reinforcement-Learning-An-Introduction,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  title = {Reinforcement Learning: An Introduction},
  edition = {2},
  publisher = {MIT Press},
  address = {Cambridge, Massachusetts},
  year = {2018}
}

@misc{Async-Methods-Deep-RL,
      title={Asynchronous Methods for Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Adrià Puigdomènech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
      year={2016},
      eprint={1602.01783},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ATARI-Deep-RL,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Mnih2015,
    author={Mnih, Volodymyr
    and Kavukcuoglu, Koray
    and Silver, David
    and Rusu, Andrei A.
    and Veness, Joel
    and Bellemare, Marc G.
    and Graves, Alex
    and Riedmiller, Martin
    and Fidjeland, Andreas K.
    and Ostrovski, Georg
    and Petersen, Stig
    and Beattie, Charles
    and Sadik, Amir
    and Antonoglou, Ioannis
    and King, Helen
    and Kumaran, Dharshan
    and Wierstra, Daan
    and Legg, Shane
    and Hassabis, Demis},
    title={Human-level control through deep reinforcement learning},
    journal={Nature},
    year={2015},
    month={Feb},
    day={01},
    volume={518},
    number={7540},
    pages={529-533},
    abstract={An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
    issn={1476-4687},
    doi={10.1038/nature14236},
    url={https://doi.org/10.1038/nature14236}
}

@article{RL-or-AIF,
    doi = {10.1371/journal.pone.0006421},
    author = {Friston, Karl J. AND Daunizeau, Jean AND Kiebel, Stefan J.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Reinforcement Learning or Active Inference?},
    year = {2009},
    month = {07},
    volume = {4},
    url = {https://doi.org/10.1371/journal.pone.0006421},
    pages = {1-13},
    abstract = {This paper questions the need for reinforcement learning or control theory when optimising behaviour. We show that it is fairly simple to teach an agent complicated and adaptive behaviours using a free-energy formulation of perception. In this formulation, agents adjust their internal states and sampling of the environment to minimize their free-energy. Such agents learn causal structure in the environment and sample it in an adaptive and self-supervised fashion. This results in behavioural policies that reproduce those optimised by reinforcement learning and dynamic programming. Critically, we do not need to invoke the notion of reward, value or utility. We illustrate these points by solving a benchmark problem in dynamic programming; namely the mountain-car problem, using active perception or inference under the free-energy principle. The ensuing proof-of-concept may be important because the free-energy formulation furnishes a unified account of both action and perception and may speak to a reappraisal of the role of dopamine in the brain.},
    number = {7},

}

@Article{AIF-Agency-Optim-Control-No-Cost-Funcs,
    author={Friston, Karl
    and Samothrakis, Spyridon
    and Montague, Read},
    title={Active inference and agency: optimal control without cost functions},
    journal={Biological Cybernetics},
    year={2012},
    month={10},
    day={01},
    volume={106},
    number={8},
    pages={523-541},
    abstract={This paper describes a variational free-energy formulation of (partially observable) Markov decision problems in decision making under uncertainty. We show that optimal control can be cast as active inference. In active inference, both action and posterior beliefs about hidden states minimise a free energy bound on the negative log-likelihood of observed states, under a generative model. In this setting, reward or cost functions are absorbed into prior beliefs about state transitions and terminal states. Effectively, this converts optimal control into a pure inference problem, enabling the application of standard Bayesian filtering techniques. We then consider optimal trajectories that rest on posterior beliefs about hidden states in the future. Crucially, this entails modelling control as a hidden state that endows the generative model with a representation of agency. This leads to a distinction between models with and without inference on hidden control states; namely, agency-free and agency-based models, respectively.},
    issn={1432-0770},
    doi={10.1007/s00422-012-0512-8},
    url={https://doi.org/10.1007/s00422-012-0512-8}
}

@article{AIF-Robotics-Artificial-Agents,
  author       = {Pablo Lanillos and
                  Cristian Meo and
                  Corrado Pezzato and
                  Ajith Anil Meera and
                  Mohamed Baioumy and
                  Wataru Ohata and
                  Alexander Tschantz and
                  Beren Millidge and
                  Martijn Wisse and
                  Christopher L. Buckley and
                  Jun Tani},
  title        = {Active Inference in Robotics and Artificial Agents: Survey and Challenges},
  journal      = {CoRR},
  volume       = {abs/2112.01871},
  year         = {2021},
  url          = {https://arxiv.org/abs/2112.01871},
  eprinttype    = {arXiv},
  eprint       = {2112.01871},
  timestamp    = {Tue, 07 Dec 2021 12:15:54 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2112-01871.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{What-Optim-About-Motor-Control,
    title = {What Is Optimal about Motor Control?},
    journal = {Neuron},
    volume = {72},
    number = {3},
    pages = {488-498},
    year = {2011},
    issn = {0896-6273},
    doi = {https://doi.org/10.1016/j.neuron.2011.10.018},
    url = {https://www.sciencedirect.com/science/article/pii/S0896627311009305},
    author = {Karl Friston},
    abstract = {This article poses a controversial question: is optimal control theory useful for understanding motor behavior or is it a misdirection? This question is becoming acute as people start to conflate internal models in motor control and perception (Poeppel et al., 2008, Hickok et al., 2011). However, the forward models in motor control are not the generative models used in perceptual inference. This Perspective tries to highlight the differences between internal models in motor control and perception and asks whether optimal control is the right way to think about things. The issues considered here may have broader implications for optimal decision theory and Bayesian approaches to learning and behavior in general.}
}

@ARTICLE{Simulating-AIF-By-Message-Passing,
    AUTHOR={van de Laar, Thijs W. and de Vries, Bert},    
    TITLE={Simulating Active Inference Processes by Message Passing},      
    JOURNAL={Frontiers in Robotics and AI},      
    VOLUME={6},           
    YEAR={2019},
    URL={https://www.frontiersin.org/articles/10.3389/frobt.2019.00020},       
    DOI={10.3389/frobt.2019.00020},      
    ISSN={2296-9144},   
    ABSTRACT={The free energy principle (FEP) offers a variational calculus-based description for how biological agents persevere through interactions with their environment. Active inference (AI) is a corollary of the FEP, which states that biological agents act to fulfill prior beliefs about preferred future observations (target priors). Purposeful behavior then results from variational free energy minimization with respect to a generative model of the environment with included target priors. However, manual derivations for free energy minimizing algorithms on custom dynamic models can become tedious and error-prone. While probabilistic programming (PP) techniques enable automatic derivation of inference algorithms on free-form models, full automation of AI requires specialized tools for inference on dynamic models, together with the description of an experimental protocol that governs the interaction between the agent and its simulated environment. The contributions of the present paper are two-fold. Firstly, we illustrate how AI can be automated with the use of ForneyLab, a recent PP toolbox that specializes in variational inference on flexibly definable dynamic models. More specifically, we describe AI agents in a dynamic environment as probabilistic state space models (SSM) and perform inference for perception and control in these agents by message passing on a factor graph representation of the SSM. Secondly, we propose a formal experimental protocol for simulated AI. We exemplify how this protocol leads to goal-directed behavior for flexibly definable AI agents in two classical RL examples, namely the Bayesian thermostat and the mountain car parking problems.}
}

@article{Factor-Graph-Approach-Automated-Design-Bayesian-Algos,
	doi = {10.1016/j.ijar.2018.11.002},
	year = 2019,
	month = {01},
	publisher = {Elsevier {BV}},
	volume = {104},
	pages = {185--204},
	author = {Marco Cox and Thijs van de Laar and Bert de Vries},
	title = {A factor graph approach to automated design of Bayesian signal processing algorithms},
	journal = {International Journal of Approximate Reasoning}
}

@article{Reactive-MP,
  author    = {Dmitry Bagaev and
               Bert de Vries},
  title     = {Reactive Message Passing for Scalable Bayesian Inference},
  journal   = {CoRR},
  volume    = {abs/2112.13251},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.13251},
  eprinttype = {arXiv},
  eprint    = {2112.13251},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-13251.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Factor-Graph-Desc-Deep-Temp-AIF,
    AUTHOR={de Vries, Bert and Friston, Karl J.},   
    TITLE={A Factor Graph Description of Deep Temporal Active Inference},      
    JOURNAL={Frontiers in Computational Neuroscience},      
    VOLUME={11},           
    YEAR={2017},        
    URL={https://www.frontiersin.org/articles/10.3389/fncom.2017.00095},       
    DOI={10.3389/fncom.2017.00095},      
    ISSN={1662-5188},   
    ABSTRACT={Active inference is a corollary of the Free Energy Principle that prescribes how self-organizing biological agents interact with their environment. The study of active inference processes relies on the definition of a generative probabilistic model and a description of how a free energy functional is minimized by neuronal message passing under that model. This paper presents a tutorial introduction to specifying active inference processes by Forney-style factor graphs (FFG). The FFG framework provides both an insightful representation of the probabilistic model and a biologically plausible inference scheme that, in principle, can be automatically executed in a computer simulation. As an illustrative example, we present an FFG for a deep temporal active inference process. The graph clearly shows how policy selection by expected free energy minimization results from free energy minimization per se, in an appropriate generative policy model.}
}

@article{Deep-AIF,
	doi = {10.1007/s00422-018-0785-7},
	howpublished = {\url{https://doi.org/10.1007\%2Fs00422-018-0785-7}},
	year = 2018,
	month = {10},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {112},
	number = {6},
	pages = {547--573},
	author = {Kai Ueltzhöffer},
	title = {Deep active inference},
	journal = {Biological Cybernetics}
}

@article{Deep-AIF-As-Var-Policy-Grad,
    author = {Millidge, Beren},
    year = {2019},
    month = {07},
    title = {Deep Active Inference as Variational Policy Gradients},
    url = {https://arxiv.org/pdf/1907.03876.pdf}
}

@phdthesis{Applications-of-FEP-Machine-Learning-Neuroscience,
  author = {Millidge, Beren},
  title = {Applications of the free energy principle to machine learning and neuroscience},
  school = {University of Edinburgh},
  year = {2021},
  url = {https://era.ed.ac.uk/handle/1842/38235}
}

@misc{kingma2022autoencoding,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{CATAL2021192_robot_nav_heir_aif,
    title = {Robot navigation as hierarchical active inference},
    journal = {Neural Networks},
    volume = {142},
    pages = {192-204},
    year = {2021},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/j.neunet.2021.05.010},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608021002021},
    author = {Ozan Çatal and Tim Verbelen and Toon {Van de Maele} and Bart Dhoedt and Adam Safron},
    keywords = {Active inference, Robot navigation, SLAM, RatSLAM, Deep learning},
    abstract = {Localization and mapping has been a long standing area of research, both in neuroscience, to understand how mammals navigate their environment, as well as in robotics, to enable autonomous mobile robots. In this paper, we treat navigation as inferring actions that minimize (expected) variational free energy under a hierarchical generative model. We find that familiar concepts like perception, path integration, localization and mapping naturally emerge from this active inference formulation. Moreover, we show that this model is consistent with models of hippocampal functions, and can be implemented in silico on a real-world robot. Our experiments illustrate that a robot equipped with our hierarchical model is able to generate topologically consistent maps, and correct navigation behaviour is inferred when a goal location is provided to the system.}
}

@InProceedings{DEEP-AIF-For-POMDPs,
    author="van der Himst, Otto
    and Lanillos, Pablo",
    editor="Verbelen, Tim
    and Lanillos, Pablo
    and Buckley, Christopher L.
    and De Boom, Cedric",
    title="Deep Active Inference for Partially Observable MDPs",
    booktitle="Active Inference",
    year="2020",
    publisher="Springer International Publishing",
    address="Cham",
    pages="61--71",
    abstract="Deep active inference has been proposed as a scalable approach to perception and action that deals with large policy and state spaces. However, current models are limited to fully observable domains. In this paper, we describe a deep active inference model that can learn successful policies directly from high-dimensional sensory inputs. The deep learning architecture optimizes a variant of the expected free energy and encodes the continuous state representation by means of a variational autoencoder. We show, in the OpenAI benchmark, that our approach has comparable or better performance than deep Q-learning, a state-of-the-art deep reinforcement learning algorithm.",
    isbn="978-3-030-64919-7"
}

@misc{Bayesian-Policy-Selection-AIF,
      title={Bayesian policy selection using active inference}, 
      author={Ozan Çatal and Johannes Nauta and Tim Verbelen and Pieter Simoens and Bart Dhoedt},
      year={2019},
      eprint={1904.08149},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@INPROCEEDINGS{Scaling-AIF,
  author={Tschantz, Alexander and Baltieri, Manuel and Seth, Anil. K. and Buckley, Christopher L.},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Scaling Active Inference}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN48605.2020.9207382}
}

@InProceedings{Infer-in-Circ-AIF-continuous-state-heriarch-gauss-filter,
    author="Waade, Peter Thestrup
    and Mikus, Nace
    and Mathys, Christoph",
    editor="Kamp, Michael
    and Koprinska, Irena
    and Bibal, Adrien
    and Bouadi, Tassadit
    and Fr{\'e}nay, Beno{\^i}t
    and Gal{\'a}rraga, Luis
    and Oramas, Jos{\'e}
    and Adilova, Linara
    and Krishnamurthy, Yamuna
    and Kang, Bo
    and Largeron, Christine
    and Lijffijt, Jefrey
    and Viard, Tiphaine
    and Welke, Pascal
    and Ruocco, Massimiliano
    and Aune, Erlend
    and Gallicchio, Claudio
    and Schiele, Gregor
    and Pernkopf, Franz
    and Blott, Michaela
    and Fr{\"o}ning, Holger
    and Schindler, G{\"u}nther
    and Guidotti, Riccardo
    and Monreale, Anna
    and Rinzivillo, Salvatore
    and Biecek, Przemyslaw
    and Ntoutsi, Eirini
    and Pechenizkiy, Mykola
    and Rosenhahn, Bodo
    and Buckley, Christopher
    and Cialfi, Daniela
    and Lanillos, Pablo
    and Ramstead, Maxwell
    and Verbelen, Tim
    and Ferreira, Pedro M.
    and Andresini, Giuseppina
    and Malerba, Donato
    and Medeiros, Ib{\'e}ria
    and Fournier-Viger, Philippe
    and Nawaz, M. Saqib
    and Ventura, Sebastian
    and Sun, Meng
    and Zhou, Min
    and Bitetta, Valerio
    and Bordino, Ilaria
    and Ferretti, Andrea
    and Gullo, Francesco
    and Ponti, Giovanni
    and Severini, Lorenzo
    and Ribeiro, Rita
    and Gama, Jo{\~a}o
    and Gavald{\`a}, Ricard
    and Cooper, Lee
    and Ghazaleh, Naghmeh
    and Richiardi, Jonas
    and Roqueiro, Damian
    and Saldana Miranda, Diego
    and Sechidis, Konstantinos
    and Gra{\c{c}}a, Guilherme",
    title="Inferring in Circles: Active Inference in Continuous State Space Using Hierarchical Gaussian Filtering of Sufficient Statistics",
    booktitle="Machine Learning and Principles and Practice of Knowledge Discovery in Databases",
    year="2021",
    publisher="Springer International Publishing",
    address="Cham",
    pages="810--818",
    abstract="We create a continuous state space active inference agent based on the hierarchical Gaussian filter. It uses the HGF to track the sufficient statistics of noisy observations of a moving target that is performing a Gaussian random walk with drift and varying volatility. On the basis of this filtering, the agent predicts the target's position, and minimizes surprisal by staying close to it. Our simulated agent represents the first full implementation of this approach. It demonstrates the feasibility of supplementing active inference with HGF-filtering of the sufficient statistics of observations, which is particularly useful in noisy and volatile continuous state space environments.",
    isbn="978-3-030-93736-2"
}

@misc{Combine-Info-Seek-Explore-and-Reward-Maximization-Under-POMDP,
      title={Combining information-seeking exploration and reward maximization: Unified inference on continuous state and action spaces under partial observability}, 
      author={Parvin Malekzadeh and Konstantinos N. Plataniotis},
      year={2022},
      eprint={2212.07946},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Reinforcement-Learning-Through-AIF,
      title={Reinforcement Learning through Active Inference}, 
      author={Alexander Tschantz and Beren Millidge and Anil K. Seth and Christopher L. Buckley},
      year={2020},
      eprint={2002.12636},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Bayesian-Policy-Selection-Using-AIF,
      title={Bayesian policy selection using active inference}, 
      author={Ozan Çatal and Johannes Nauta and Tim Verbelen and Pieter Simoens and Bart Dhoedt},
      year={2019},
      eprint={1904.08149},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Contrastive-AIF,
    author = {Mazzaglia, Pietro and Verbelen, Tim and Dhoedt, Bart},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages = {13870--13882},
    publisher = {Curran Associates, Inc.},
    title = {Contrastive Active Inference},
    url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/73c730319cf839f143bf40954448ce39-Paper.pdf},
    volume = {34},
    year = {2021}
}

@ARTICLE{AIDA,
    AUTHOR={Podusenko, Albert and van Erp, Bart and Koudahl, Magnus and de Vries, Bert},
    TITLE={AIDA: An Active Inference-Based Design Agent for Audio Processing Algorithms},
    JOURNAL={Frontiers in Signal Processing},
    VOLUME={2},
    YEAR={2022},
    URL={https://www.frontiersin.org/articles/10.3389/frsip.2022.842477},
    DOI={10.3389/frsip.2022.842477},
    ISSN={2673-8198},
    ABSTRACT={In this paper we present Active Inference-Based Design Agent (AIDA), which is an active inference-based agent that iteratively designs a personalized audio processing algorithm through situated interactions with a human client. The target application of AIDA is to propose on-the-spot the most interesting alternative values for the tuning parameters of a hearing aid (HA) algorithm, whenever a HA client is not satisfied with their HA performance. AIDA interprets searching for the “most interesting alternative” as an issue of optimal (acoustic) context-aware Bayesian trial design. In computational terms, AIDA is realized as an active inference-based agent with an Expected Free Energy criterion for trial design. This type of architecture is inspired by neuro-economic models on efficient (Bayesian) trial design in brains and implies that AIDA comprises generative probabilistic models for acoustic signals and user responses. We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive filters and a user response model based on a Gaussian Process Classifier. The full AIDA agent has been implemented in a factor graph for the generative model and all tasks (parameter learning, acoustic context classification, trial design, etc.) are realized by variational message passing on the factor graph. All verification and validation experiments and demonstrations are freely accessible at our GitHub repository.}
}

@article{Mazzaglia_2022,
    doi = {10.3390/e24020301},
    url = {https://doi.org/10.3390%2Fe24020301},
    year = 2022,
    month = {02},
    publisher = {{MDPI} {AG}},
    volume = {24},
    number = {2},
    pages = {301},
    author = {Pietro Mazzaglia and Tim Verbelen and Ozan {\c{C}}atal and Bart Dhoedt},
    title = {The Free Energy Principle for Perception and Action: A Deep Learning Perspective},
    journal = {Entropy}
}

@misc{towers_gymnasium_2023,
        title = {Gymnasium},
        url = {https://zenodo.org/record/8127025},
        abstract = {An API standard for single-agent reinforcement learning environments, with popular reference environments and related utilities (formerly Gym)},
        urldate = {2023-07-08},
        publisher = {Zenodo},
        author = {Towers, Mark and Terry, Jordan K. and Kwiatkowski, Ariel and Balis, John U. and Cola, Gianluca de and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Shen, Andrew Tan Jin and Younis, Omar G.},
        month = mar,
        year = {2023},
        doi = {10.5281/zenodo.8127026},
}

@article{CULLEN_actinf_openai_gym,
    title = {Active Inference in OpenAI Gym: A Paradigm for Computational Investigations Into Psychiatric Illness},
    journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
    volume = {3},
    number = {9},
    pages = {809-818},
    year = {2018},
    note = {Computational Methods and Modeling in Psychiatry},
    issn = {2451-9022},
    doi = {https://doi.org/10.1016/j.bpsc.2018.06.010},
    url = {https://www.sciencedirect.com/science/article/pii/S2451902218301617},
    author = {Maell Cullen and Ben Davey and Karl J. Friston and Rosalyn J. Moran},
    keywords = {Active inference, Computational phenotyping, Computational psychiatry, Free energy principle, Game-based imaging biomarkers, Markov decision process},
    abstract = {Background
    Artificial intelligence has recently attained humanlike performance in a number of gamelike domains. These advances have been spurred by brain-inspired architectures and algorithms such as hierarchical filtering and reinforcement learning. OpenAI Gym is an open-source platform in which to train, test, and benchmark algorithms—it provides a range of tasks, including those of classic arcade games such as Doom. Here we describe how the platform might be used as a simulation, test, and diagnostic paradigm for psychiatric conditions.
    Methods
    To illustrate how active inference models of game play could be used to test mechanistic and algorithmic properties of psychiatric disorders, we provide two exemplar analyses. The first speaks to the impact of aging on cognition, examining game-play behaviors in a model of aging in which we compared age-dependent changes of younger (n = 9, 22 ± 1 years of age) and older (n = 7, 56 ± 5 years of age) adult players. The second is an illustration of a putative feature of anhedonia in which we simulated diminished sensitivity to reward.
    Results
    These simulations demonstrate how active inference can be used to test predicted changes in both neurobiology and beliefs in psychiatric cohorts. We show that, as well as behavioral measures, putative neural correlates of active inference can be simulated, and hypothesized (model-based) differences in local field potentials and blood oxygen level–dependent responses can be produced.
    Conclusions
    We show that active inference, through epistemic and value-based goals, enables simulated subjects to actively develop detailed representations of gaming environments, and we demonstrate the use of a principled algorithmic and neurobiological framework for testing hypotheses in psychiatric illness.}
}

@software{Minigrid,
  author = {Chevalier-Boisvert, Maxime and Willems, Lucas and Pal, Suman},
  title = {Minimalistic Gridworld Environment for Gymnasium},
  url = {https://github.com/Farama-Foundation/Minigrid},
  year = {2018},
}

@article{Deep-Mind-Control-Suite,
         title = {dm_control: Software and tasks for continuous control},
         journal = {Software Impacts},
         volume = {6},
         pages = {100022},
         year = {2020},
         issn = {2665-9638},
         doi = {https://doi.org/10.1016/j.simpa.2020.100022},
         url = {https://www.sciencedirect.com/science/article/pii/S2665963820300099},
         author = {Saran Tunyasuvunakool and Alistair Muldal and Yotam Doron and
                   Siqi Liu and Steven Bohez and Josh Merel and Tom Erez and
                   Timothy Lillicrap and Nicolas Heess and Yuval Tassa},
}

@InProceedings{Message-Passing-Perspective-Planning-Under-AIF,
    author="Koudahl, Magnus
    and Buckley, Christopher L.
    and de Vries, Bert",
    editor="Buckley, Christopher L.
    and Cialfi, Daniela
    and Lanillos, Pablo
    and Ramstead, Maxwell
    and Sajid, Noor
    and Shimazaki, Hideaki
    and Verbelen, Tim",
    title="A Message Passing Perspective on Planning Under Active Inference",
    booktitle="Active Inference",
    year="2023",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="319--327",
    abstract="We present a message passing interpretation of planning under Active Inference. Specifically, we show how the Active Inference planning procedure can be broken into a (partial) message passing sweep over a graph, followed by local computations of a cost functional (the Expected Free Energy). Using Forney-style Factor Graphs, we then proceed to show how one can derive novel planning schemes by local changes to the underlying graph and message passing schedule. We illustrate this by first isolating the ``sophisticated'' aspect of Sophisticated Inference and then proposing a novel planning algorithm through combining the sophisticated update mechanism with a different message passing schedule. Our main contribution is a modular view of planning under Active Inference that can serve as a framework for both understanding existing algorithms, deriving new ones and extending the class of models that are amenable to Active Inference. Approaching Active Inference from a message passing perspective also shows how it can be efficiently implemented using off-the-shelf probabilistic programming software, broadening the class of models available to researchers and practitioners.",
    isbn="978-3-031-28719-0"
}

@article{Ueltzh_ffer_2018,
    doi = {10.1007/s00422-018-0785-7},
    url = {https://doi.org/10.1007%2Fs00422-018-0785-7},
    year = 2018,
    month = {10},
    publisher = {Springer Science and Business Media {LLC}},
    volume = {112},
    number = {6},
    pages = {547--573},
    author = {Kai Ueltzhöffer},
    title = {Deep active inference},
    journal = {Biological Cybernetics}
}

@unknown{Relationship-Dynamic-Programming-AIF,
    author = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston, Karl and Smith, Ryan},
    year = {2020},
    month = {09},
    title = {The relationship between dynamic programming and active inference: the discrete, finite-horizon case}
}

@article{RLflawed,
  author = {Christiano, Paul},
  title = {Why Reinforcement Learning is Flawed},
  journal = {The Gradient},
  year = {2019},
  url = {https://thegradient.pub/why-rl-is-flawed/}
}

@article{Uncertainty_Epistemics_AIF_Saccad,
author = {Parr, Thomas  and Friston, Karl J. },
title = {Uncertainty, epistemics and active inference},
journal = {Journal of The Royal Society Interface},
volume = {14},
number = {136},
pages = {20170376},
year = {2017},
doi = {10.1098/rsif.2017.0376},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2017.0376},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0376}
,
    abstract = { Biological systems—like ourselves—are constantly faced with uncertainty. Despite noisy sensory data, and volatile environments, creatures appear to actively maintain their integrity. To account for this remarkable ability to make optimal decisions in the face of a capricious world, we propose a generative model that represents the beliefs an agent might possess about their own uncertainty. By simulating a noisy and volatile environment, we demonstrate how uncertainty influences optimal epistemic (visual) foraging. In our simulations, saccades were deployed less frequently to regions with a lower sensory precision, while a greater volatility led to a shorter inhibition of return. These simulations illustrate a principled explanation for some cardinal aspects of visual foraging—and allow us to propose a correspondence between the representation of uncertainty and ascending neuromodulatory systems, complementing that suggested by Yu \&amp; Dayan (Yu \&amp; Dayan 2005 Neuron 46, 681–692. (doi:10.1016/j.neuron.2005.04.026)). }
}


@InProceedings{Curiosity-Driven-RL,
  title =    {Curiosity-driven Exploration by Self-supervised Prediction},
  author =       {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  booktitle =    {Proceedings of the 34th International Conference on Machine Learning},
  pages =    {2778--2787},
  year =     {2017},
  editor =   {Precup, Doina and Teh, Yee Whye},
  volume =   {70},
  series =   {Proceedings of Machine Learning Research},
  month =    {08},
  publisher =    {PMLR},
  pdf =      {http://proceedings.mlr.press/v70/pathak17a/pathak17a.pdf},
  url =      {https://proceedings.mlr.press/v70/pathak17a.html},
  abstract =     {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent’s ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.}
}

@Article{Generalized-Filtering,
    author={Friston, Karl
    and Stephan, Klaas
    and Li, Baojuan
    and Daunizeau, Jean},
    title={Generalised Filtering},
    journal={Mathematical Problems in Engineering},
    year={2010},
    month={06},
    day={27},
    publisher={Hindawi Publishing Corporation},
    volume={2010},
    pages={621670},
    abstract={We describe a Bayesian filtering scheme for nonlinear state-space models in continuous time. This scheme is called Generalised Filtering and furnishes posterior (conditional) densities on hidden states and unknown parameters generating observed data. Crucially, the scheme operates online, assimilating data to optimize the conditional density on time-varying states and time-invariant parameters. In contrast to Kalman and Particle smoothing, Generalised Filtering does not require a backwards pass.  In contrast to variational schemes, it does not assume conditional independence between the states and parameters. Generalised Filtering optimises the conditional density with respect to a free-energy bound on the model's log-evidence. This optimisation uses the generalised motion of hidden states and parameters, under the prior assumption that the motion of the parameters is small. We describe the scheme, present comparative evaluations with a fixed-form variational version, and conclude with an illustrative application to a nonlinear state-space model of brain imaging time-series.},
    issn={1024-123X},
    doi={10.1155/2010/621670},
    url={https://doi.org/10.1155/2010/621670}
}

@article{DEM,
    title = {DEM: A variational treatment of dynamic systems},
    journal = {NeuroImage},
    volume = {41},
    number = {3},
    pages = {849-885},
    year = {2008},
    issn = {1053-8119},
    doi = {https://doi.org/10.1016/j.neuroimage.2008.02.054},
    url = {https://www.sciencedirect.com/science/article/pii/S1053811908001894},
    author = {K.J. Friston and N. Trujillo-Barreto and J. Daunizeau},
    keywords = {Variational Bayes, Free energy, Action, Dynamic expectation maximisation, Dynamical systems, Nonlinear, Bayesian filtering, Variational filtering},
    abstract = {This paper presents a variational treatment of dynamic models that furnishes time-dependent conditional densities on the path or trajectory of a system's states and the time-independent densities of its parameters. These are obtained by maximising a variational action with respect to conditional densities, under a fixed-form assumption about their form. The action or path-integral of free-energy represents a lower bound on the model's log-evidence or marginal likelihood required for model selection and averaging. This approach rests on formulating the optimisation dynamically, in generalised coordinates of motion. The resulting scheme can be used for online Bayesian inversion of nonlinear dynamic causal models and is shown to outperform existing approaches, such as Kalman and particle filtering. Furthermore, it provides for dual and triple inferences on a system's states, parameters and hyperparameters using exactly the same principles. We refer to this approach as dynamic expectation maximisation (DEM).}
}

@InProceedings{UCT,
author="Kocsis, Levente
and Szepesv{\'a}ri, Csaba",
editor="F{\"u}rnkranz, Johannes
and Scheffer, Tobias
and Spiliopoulou, Myra",
title="Bandit Based Monte-Carlo Planning",
booktitle="Machine Learning: ECML 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="282--293",
abstract="For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.",
isbn="978-3-540-46056-5"
}



@inproceedings{Fountas_AIFMCTS,
 author = {Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro and Friston, Karl},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {11662--11675},
 publisher = {Curran Associates, Inc.},
 title = {Deep active inference agents using Monte-Carlo methods},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/865dfbde8a344b44095495f3591f7407-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{KAELBLING199899,
    title = {Planning and acting in partially observable stochastic domains},
    journal = {Artificial Intelligence},
    volume = {101},
    number = {1},
    pages = {99-134},
    year = {1998},
    issn = {0004-3702},
    doi = {https://doi.org/10.1016/S0004-3702(98)00023-X},
    url = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
    author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
    keywords = {Planning, Uncertainty, Partially observable Markov decision processes},
    abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.}
}

@article{aastrom1965optimal,
  title={Optimal control of Markov processes with incomplete state information I},
  author={{\AA}str{\"o}m, Karl Johan},
  journal={Journal of mathematical analysis and applications},
  volume={10},
  pages={174--205},
  year={1965},
  publisher={Elsevier}
}

@article{BTAI_Theory,
    title = {Branching Time Active Inference: The theory and its generality},
    journal = {Neural Networks},
    volume = {151},
    pages = {295-316},
    year = {2022},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/j.neunet.2022.03.036},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608022001149},
    author = {Théophile Champion and Lancelot {Da Costa} and Howard Bowman and Marek Grześ},
    keywords = {Active inference, Variational message passing, Tree search, Planning, Free energy principle},
    abstract = {Over the last 10 to 15 years, active inference has helped to explain various brain mechanisms from habit formation to dopaminergic discharge and even modelling curiosity. However, the current implementations suffer from an exponential (space and time) complexity class when computing the prior over all the possible policies up to the time-horizon. Fountas et al. (2020) used Monte Carlo tree search to address this problem, leading to impressive results in two different tasks. In this paper, we present an alternative framework that aims to unify tree search and active inference by casting planning as a structure learning problem. Two tree search algorithms are then presented. The first propagates the expected free energy forward in time (i.e., towards the leaves), while the second propagates it backward (i.e., towards the root). Then, we demonstrate that forward and backward propagations are related to active inference and sophisticated inference, respectively, thereby clarifying the differences between those two planning strategies.}
}

@article{Branch-Time-AIF,
    title = {Branching time active inference: Empirical study and complexity class analysis},
    journal = {Neural Networks},
    volume = {152},
    pages = {450-466},
    year = {2022},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/j.neunet.2022.05.010},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608022001824},
    author = {Théophile Champion and Howard Bowman and Marek Grześ},
    keywords = {Active inference, Variational message passing, Tree search, Planning, Free energy principle},
    abstract = {Active inference is a state-of-the-art framework for modelling the brain that explains a wide range of mechanisms such as habit formation, dopaminergic discharge and curiosity. However, recent implementations suffer from an exponential (space and time) complexity class when computing the prior over all the possible policies up to the time horizon. Fountas et al. (2020) used Monte Carlo tree search to address this problem, leading to very good results in two different tasks. Additionally, Champion et al. (2021a) proposed a tree search approach based on (temporal) structure learning. This was enabled by the development of a variational message passing approach to active inference (Champion, Bowman, Grześ, 2021), which enables compositional construction of Bayesian networks for active inference. However, this message passing tree search approach, which we call branching-time active inference (BTAI), has never been tested empirically. In this paper, we present an experimental study of the approach (Champion, Grześ, Bowman, 2021) in the context of a maze solving agent. In this context, we show that both improved prior preferences and deeper search help mitigate the vulnerability to local minima. Then, we compare BTAI to standard active inference (AcI) on a graph navigation task. We show that for small graphs, both BTAI and AcI successfully solve the task. For larger graphs, AcI exhibits an exponential (space) complexity class, making the approach intractable. However, BTAI explores the space of policies more efficiently, successfully scaling to larger graphs. Then, BTAI was compared to the POMCP algorithm (Silver and Veness, 2010) on the frozen lake environment. The experiments suggest that BTAI and the POMCP algorithm accumulate a similar amount of reward. Also, we describe when BTAI receives more rewards than the POMCP agent, and when the opposite is true. Finally, we compared BTAI to the approach of Fountas et al. (2020) on the dSprites dataset, and we discussed the pros and cons of each approach.}
}

@misc{champion2023deconstructing,
      title={Deconstructing deep active inference}, 
      author={Théophile Champion and Marek Grześ and Lisa Bonheme and Howard Bowman},
      year={2023},
      eprint={2303.01618},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@inproceedings{Bayes-State-Estimation,
    author = {Bhashyam Balaji and Karl Friston},
    title = {{Bayesian state estimation using generalized coordinates}},
    volume = {8050},
    booktitle = {Signal Processing, Sensor Fusion, and Target Recognition XX},
    editor = {Ivan Kadar},
    organization = {International Society for Optics and Photonics},
    publisher = {SPIE},
    pages = {80501Y},
    keywords = {Variational Bayes, Variational Filtering, Generalized Coordinates, Dynamical Causal Modelling, Hierarchical Dynamical Model, Continuous-Discrete Filtering, Kolmogorov Equation, Fokker-Planck Equation},
    year = {2011},
    doi = {10.1117/12.883513},
    URL = {https://doi.org/10.1117/12.883513}
}

@ARTICLE{Intro-to-Factor-Graphs,
  author={Loeliger, H.-A.},
  journal={IEEE Signal Processing Magazine}, 
  title={An introduction to factor graphs}, 
  year={2004},
  volume={21},
  number={1},
  pages={28-41},
  doi={10.1109/MSP.2004.1267047}
}

@ARTICLE{Learn-Gen-State-Space-Models-AIF-reeee,
    AUTHOR={Çatal, Ozan and Wauthier, Samuel and De Boom, Cedric and Verbelen, Tim and Dhoedt, Bart},   
    TITLE={Learning Generative State Space Models for Active Inference},      
    JOURNAL={Frontiers in Computational Neuroscience},      
    VOLUME={14},           
    YEAR={2020},      
    URL={https://www.frontiersin.org/articles/10.3389/fncom.2020.574372},       
    DOI={10.3389/fncom.2020.574372},      
    ISSN={1662-5188},   
    ABSTRACT={In this paper we investigate the active inference framework as a means to enable autonomous behavior in artificial agents. Active inference is a theoretical framework underpinning the way organisms act and observe in the real world. In active inference, agents act in order to minimize their so called free energy, or prediction error. Besides being biologically plausible, active inference has been shown to solve hard exploration problems in various simulated environments. However, these simulations typically require handcrafting a generative model for the agent. Therefore we propose to use recent advances in deep artificial neural networks to learn generative state space models from scratch, using only observation-action sequences. This way we are able to scale active inference to new and challenging problem domains, whilst still building on the theoretical backing of the free energy principle. We validate our approach on the mountain car problem to illustrate that our learnt models can indeed trade-off instrumental value and ambiguity. Furthermore, we show that generative models can also be learnt using high-dimensional pixel observations, both in the OpenAI Gym car racing environment and a real-world robotic navigation task. Finally we show that active inference based policies are an order of magnitude more sample efficient than Deep Q Networks on RL tasks.}
}

@ARTICLE{learn_gen_ssm_aif,
    AUTHOR={Çatal, Ozan and Wauthier, Samuel and De Boom, Cedric and Verbelen, Tim and Dhoedt, Bart},
    TITLE={Learning Generative State Space Models for Active Inference},
    JOURNAL={Frontiers in Computational Neuroscience},
    VOLUME={14},
    YEAR={2020},
    URL={https://www.frontiersin.org/articles/10.3389/fncom.2020.574372},     
    DOI={10.3389/fncom.2020.574372},
    ISSN={1662-5188},
    ABSTRACT={In this paper we investigate the active inference framework as a means to enable autonomous behavior in artificial agents. Active inference is a theoretical framework underpinning the way organisms act and observe in the real world. In active inference, agents act in order to minimize their so called free energy, or prediction error. Besides being biologically plausible, active inference has been shown to solve hard exploration problems in various simulated environments. However, these simulations typically require handcrafting a generative model for the agent. Therefore we propose to use recent advances in deep artificial neural networks to learn generative state space models from scratch, using only observation-action sequences. This way we are able to scale active inference to new and challenging problem domains, whilst still building on the theoretical backing of the free energy principle. We validate our approach on the mountain car problem to illustrate that our learnt models can indeed trade-off instrumental value and ambiguity. Furthermore, we show that generative models can also be learnt using high-dimensional pixel observations, both in the OpenAI Gym car racing environment and a real-world robotic navigation task. Finally we show that active inference based policies are an order of magnitude more sample efficient than Deep Q Networks on RL tasks.}
}

@Article{Neural-Dynamics-AIF,
    AUTHOR = {Da Costa, Lancelot and Parr, Thomas and Sengupta, Biswa and Friston, Karl},
    TITLE = {Neural Dynamics under Active Inference: Plausibility and Efficiency of Information Processing},
    JOURNAL = {Entropy},
    VOLUME = {23},
    YEAR = {2021},
    NUMBER = {4},
    ARTICLE-NUMBER = {454},
    URL = {https://www.mdpi.com/1099-4300/23/4/454},
    PubMedID = {33921298},
    ISSN = {1099-4300},
    ABSTRACT = {Active inference is a normative framework for explaining behaviour under the free energy principle—a theory of self-organisation originating in neuroscience. It specifies neuronal dynamics for state-estimation in terms of a descent on (variational) free energy—a measure of the fit between an internal (generative) model and sensory observations. The free energy gradient is a prediction error—plausibly encoded in the average membrane potentials of neuronal populations. Conversely, the expected probability of a state can be expressed in terms of neuronal firing rates. We show that this is consistent with current models of neuronal dynamics and establish face validity by synthesising plausible electrophysiological responses. We then show that these neuronal dynamics approximate natural gradient descent, a well-known optimisation algorithm from information geometry that follows the steepest descent of the objective in information space. We compare the information length of belief updating in both schemes, a measure of the distance travelled in information space that has a direct interpretation in terms of metabolic cost. We show that neural dynamics under active inference are metabolically efficient and suggest that neural representations in biological agents may evolve by approximating steepest descent in information space towards the point of optimal inference.},
    DOI = {10.3390/e23040454}
}

@misc{ChatGPT-4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{AIF-Revo-Robot,
   author = {{Da Costa}, Lancelot and {Lanillos}, Pablo and {Sajid}, Noor and {Friston}, Karl and {Khan}, Shujhat},
    title = "{How Active Inference Could Help Revolutionise Robotics}",
  journal = {Entropy},
     year = 2022,
    month = mar,
   volume = {24},
   number = {3},
    pages = {361},
      doi = {10.3390/e24030361},
   adsurl = {https://ui.adsabs.harvard.edu/abs/2022Entrp..24..361D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{AcT,
      title={Active Inference Tree Search in Large POMDPs}, 
      author={Domenico Maisto and Francesco Gregoretti and Karl Friston and Giovanni Pezzulo},
      year={2023},
      eprint={2103.13860},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@ARTICLE{Whence_EFE,
  author={Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
  journal={Neural Computation}, 
  title={Whence the Expected Free Energy?}, 
  year={2021},
  volume={33},
  number={2},
  pages={447-482},
  doi={10.1162/neco_a_01354}
}
